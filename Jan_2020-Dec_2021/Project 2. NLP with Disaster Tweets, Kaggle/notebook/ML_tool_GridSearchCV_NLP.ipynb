{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import nltk\n","import string\n","import numpy as np\n","import pandas as pd\n","\n","from nltk.corpus import stopwords\n","from nltk.util import ngrams\n","from sklearn.feature_extraction.text import CountVectorizer\n","from collections import defaultdict, Counter\n","\n","import re\n","from nltk.tokenize import word_tokenize\n","import gensim\n","import string\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from tqdm import tqdm\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["### Loading & viewing dataset.\n","- References [link_1](https://github.com/Nhan121/Kaggle-6-first-projects/blob/master/NLP_Text_Classification/NLP_Text_classification.ipynb)"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id keyword location                                               text  \\\n","0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n","1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n","2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n","3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n","4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n","\n","   target  \n","0       1  \n","1       1  \n","2       1  \n","3       1  \n","4       1  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["path = r'../input/nlp-getting-started'\n","train = pd.read_csv(path + '/train.csv')\n","train.head()"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Pre-processing\n","### 1.1. Counting & handling missing-values"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count_NA</th>\n","      <th>%NA</th>\n","      <th>type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>id</th>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>int64</td>\n","    </tr>\n","    <tr>\n","      <th>keyword</th>\n","      <td>61</td>\n","      <td>0.80</td>\n","      <td>object</td>\n","    </tr>\n","    <tr>\n","      <th>location</th>\n","      <td>2533</td>\n","      <td>33.27</td>\n","      <td>object</td>\n","    </tr>\n","    <tr>\n","      <th>text</th>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>object</td>\n","    </tr>\n","    <tr>\n","      <th>target</th>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>int64</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          count_NA    %NA    type\n","id               0   0.00   int64\n","keyword         61   0.80  object\n","location      2533  33.27  object\n","text             0   0.00  object\n","target           0   0.00   int64"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["pd.DataFrame({'count_NA': train.isna().sum(), \n","              '%NA': round(100*train.isna().sum() / train.shape[0], 2), \n","              'type' :train.dtypes})"]},{"cell_type":"markdown","metadata":{},"source":["#### Comment.\n","- For the column `keyword`, it takes about `0.8%` at both dataset; whilethe column `location` is more than `33%`. Both of them (both columns) are the `object` types so we can replace the missing values by `unknown` to keep the structure preservation.\n","- The rest columns `id, text, target` (in `train.csv`) and `id, text` (in `test.csv`) has no missing value since we will predict the `target` (and add them to the `test`) mainly based on the `text`."]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>unknown</td>\n","      <td>unknown</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>unknown</td>\n","      <td>unknown</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>unknown</td>\n","      <td>unknown</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>unknown</td>\n","      <td>unknown</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>unknown</td>\n","      <td>unknown</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  keyword location                                               text  \\\n","0   1  unknown  unknown  Our Deeds are the Reason of this #earthquake M...   \n","1   4  unknown  unknown             Forest fire near La Ronge Sask. Canada   \n","2   5  unknown  unknown  All residents asked to 'shelter in place' are ...   \n","3   6  unknown  unknown  13,000 people receive #wildfires evacuation or...   \n","4   7  unknown  unknown  Just got sent this photo from Ruby #Alaska as ...   \n","\n","   target  \n","0       1  \n","1       1  \n","2       1  \n","3       1  \n","4       1  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train = train.fillna('unknown')\n","train.head()"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2. Droping duplicated-values\n","#### The unique-values."]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cnt_uniq</th>\n","      <th>perc_uniq_%</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>id</th>\n","      <td>7613</td>\n","      <td>100.00</td>\n","    </tr>\n","    <tr>\n","      <th>keyword</th>\n","      <td>222</td>\n","      <td>2.92</td>\n","    </tr>\n","    <tr>\n","      <th>location</th>\n","      <td>3342</td>\n","      <td>43.90</td>\n","    </tr>\n","    <tr>\n","      <th>text</th>\n","      <td>7503</td>\n","      <td>98.56</td>\n","    </tr>\n","    <tr>\n","      <th>target</th>\n","      <td>2</td>\n","      <td>0.03</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          cnt_uniq  perc_uniq_%\n","id            7613       100.00\n","keyword        222         2.92\n","location      3342        43.90\n","text          7503        98.56\n","target           2         0.03"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["count_unique = [len(train[col].unique()) for col in train.columns]\n","percent_uniq = [round(100*cnt / train.shape[0], 2) for cnt in count_unique]\n","pd.DataFrame({'cnt_uniq': count_unique, 'perc_uniq_%': percent_uniq}, index = train.columns)"]},{"cell_type":"markdown","metadata":{},"source":["#### Droping the duplicates values (after ignore the `id`)"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["(7561, 4)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["train_non_id = train.drop(columns = ['id'])\n","train_non_id = train_non_id.drop_duplicates()\n","train_non_id.shape"]},{"cell_type":"markdown","metadata":{},"source":["### 1.3. Text-processing."]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pyspellchecker\n","  Downloading pyspellchecker-0.5.6-py2.py3-none-any.whl (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 3.0 MB/s eta 0:00:01\n","\u001b[?25hInstalling collected packages: pyspellchecker\n","Successfully installed pyspellchecker-0.5.6\n","\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.3.3 is available.\n","You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"]}],"source":["!pip install pyspellchecker"]},{"cell_type":"markdown","metadata":{},"source":["The following function is prepared for the Section 3.2 (Model using `text-processing`)"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 7.53 ms, sys: 924 µs, total: 8.46 ms\n","Wall time: 8.39 ms\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>correct_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>unknown</td>\n","      <td>unknown</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>unknown</td>\n","      <td>unknown</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>unknown</td>\n","      <td>unknown</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","      <td>[all, residents, asked, to, shelter, in, place...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>unknown</td>\n","      <td>unknown</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","      <td>[13, 000, people, receive, wildfires, evacuati...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>unknown</td>\n","      <td>unknown</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  keyword location                                               text  \\\n","0   1  unknown  unknown  Our Deeds are the Reason of this #earthquake M...   \n","1   4  unknown  unknown             Forest fire near La Ronge Sask. Canada   \n","2   5  unknown  unknown  All residents asked to 'shelter in place' are ...   \n","3   6  unknown  unknown  13,000 people receive #wildfires evacuation or...   \n","4   7  unknown  unknown  Just got sent this photo from Ruby #Alaska as ...   \n","\n","   target                                       correct_text  \n","0       1  [our, deeds, are, the, reason, of, this, earth...  \n","1       1      [forest, fire, near, la, ronge, sask, canada]  \n","2       1  [all, residents, asked, to, shelter, in, place...  \n","3       1  [13, 000, people, receive, wildfires, evacuati...  \n","4       1  [just, got, sent, this, photo, from, ruby, ala...  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["from pre_process import *\n","## Review the first 5 lines after using text-processing\n","new_train = train.copy()\n","%time new_train.loc[:5, 'correct_text'] = train.loc[:5, 'text'].apply(lambda x: process_text(x))\n","new_train.head()"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Grid-Seach CV.\n","### 2.1. Initialize the model"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>Text_length</th>\n","      <th>Numb_words</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>unknown</td>\n","      <td>unknown</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","      <td>69</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>unknown</td>\n","      <td>unknown</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","      <td>38</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>unknown</td>\n","      <td>unknown</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","      <td>133</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>unknown</td>\n","      <td>unknown</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","      <td>65</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>unknown</td>\n","      <td>unknown</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","      <td>88</td>\n","      <td>16</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    keyword location                                               text  \\\n","id                                                                        \n","1   unknown  unknown  Our Deeds are the Reason of this #earthquake M...   \n","4   unknown  unknown             Forest fire near La Ronge Sask. Canada   \n","5   unknown  unknown  All residents asked to 'shelter in place' are ...   \n","6   unknown  unknown  13,000 people receive #wildfires evacuation or...   \n","7   unknown  unknown  Just got sent this photo from Ruby #Alaska as ...   \n","\n","    target  Text_length  Numb_words  \n","id                                   \n","1        1           69          13  \n","4        1           38           7  \n","5        1          133          22  \n","6        1           65           8  \n","7        1           88          16  "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train['Text_length'] = train['text'].str.len()\n","train['Numb_words'] = train['text'].str.split().map(lambda x: len(x))\n","train = train.set_index('id')\n","train.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Train-test split.\n","- 2 columns `keyword, location` has so many categories so that it can not be used to establish the model.\n","- It make more time in computation and the efficients is not good."]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["# Initialize the tfidf_vectorizer \n","from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf_vectorizer = TfidfVectorizer(stop_words = 'english') \n","X = tfidf_vectorizer.fit_transform(train['text']) \n","\n","## Target\n","y = train['target']"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["((5329, 21363), (2284, 21363), (5329,))"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.3, random_state=42)\n","\n","X_train.shape, X_test.shape, y_train.shape"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2. Lauching with SVM model"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 3min 25s, sys: 3.44 s, total: 3min 29s\n","Wall time: 3min 29s\n"]},{"data":{"text/plain":["GridSearchCV(cv=8, estimator=SVC(),\n","             param_grid=[{'C': [0.1, 1, 5],\n","                          'kernel': ['linear', 'rbf', 'poly']}])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn import svm\n","from sklearn.model_selection import GridSearchCV\n","\n","svm = svm.SVC()\n","grid_params = [{\n","                'kernel':['linear', 'rbf', 'poly'],\n","                'C': [0.1, 1, 5], #default: 1.0\n","                 }]\n","clf_svm = GridSearchCV(estimator=svm, param_grid = grid_params, cv = 8, verbose = 0)\n","%time clf_svm.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["{'C': 1, 'kernel': 'linear'}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["clf_svm.best_params_"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["0.7907679043361202"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["clf_svm.best_score_"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.9502720960780634\n","0.8104203152364273\n"]}],"source":["from sklearn.metrics import accuracy_score\n","\n","pred_train = clf_svm.predict(X_train)\n","pred_test = clf_svm.predict(X_test)\n","\n","print(accuracy_score(y_train, pred_train))\n","print(accuracy_score(y_test, pred_test))"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[2994   45]\n"," [ 220 2070]]\n","[[1178  125]\n"," [ 308  673]]\n"]}],"source":["from sklearn.metrics import confusion_matrix\n","print(confusion_matrix(y_train, pred_train))\n","print(confusion_matrix(y_test, pred_test))"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>best_params</th>\n","      <th>train_acc_%</th>\n","      <th>train_conf_matrix</th>\n","      <th>test_acc_%</th>\n","      <th>test_conf_matrix</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>SVM</th>\n","      <td>{'C': 1, 'kernel': 'linear'}</td>\n","      <td>95.02721</td>\n","      <td>[[2994, 45], [220, 2070]]</td>\n","      <td>81.042032</td>\n","      <td>[[1178, 125], [308, 673]]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                      best_params  train_acc_%          train_conf_matrix  \\\n","SVM  {'C': 1, 'kernel': 'linear'}     95.02721  [[2994, 45], [220, 2070]]   \n","\n","     test_acc_%           test_conf_matrix  \n","SVM   81.042032  [[1178, 125], [308, 673]]  "]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.DataFrame(columns = ['best_params', 'train_acc_%', 'train_conf_matrix', 'test_acc_%', 'test_conf_matrix'])\n","df.loc['SVM'] = [clf_svm.best_params_, \n","                 100*accuracy_score(y_train, pred_train), confusion_matrix(y_train, pred_train), \n","                 100*accuracy_score(y_test, pred_test), confusion_matrix(y_test, pred_test)]\n","df"]},{"cell_type":"markdown","metadata":{},"source":["## 3. And for another model"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["((5329, 17330), (2284, 17330), (5329,))"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import StandardScaler\n","X_train_num = train[['Text_length', 'Numb_words']].to_numpy()\n","X_con = StandardScaler().fit_transform(X_train_num)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","X_C = CountVectorizer(analyzer = process_text).fit_transform(train['text'])\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_C, y, stratify = y, test_size=0.3, random_state=0)\n","\n","X_train.shape, X_test.shape, y_train.shape"]},{"cell_type":"markdown","metadata":{},"source":["### 3.1. Naive Bayes"]},{"cell_type":"code","execution_count":20,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 93.4 ms, sys: 0 ns, total: 93.4 ms\n","Wall time: 93.2 ms\n","{'alpha': 1}\n","0.903921936573466\n","0.8025394045534151\n"]}],"source":["from sklearn.naive_bayes import MultinomialNB, GaussianNB\n","\n","naiv = MultinomialNB()\n","grid_params = [{'alpha' : [0.5, 0.75, 0.8, 1]}]\n","clf = GridSearchCV(estimator=naiv, param_grid = grid_params, cv = 5, verbose = 0)\n","%time clf.fit(X_train, y_train)\n","\n","pred_train = clf.predict(X_train)\n","pred_test = clf.predict(X_test)\n","\n","print(clf.best_params_)\n","print(accuracy_score(y_train, pred_train))\n","print(accuracy_score(y_test, pred_test))\n","\n","df.loc['MultiNB'] = [clf.best_params_, \n","                     100*accuracy_score(y_train, pred_train), confusion_matrix(y_train, pred_train), \n","                     100*accuracy_score(y_test, pred_test), confusion_matrix(y_test, pred_test)]"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2. Logistic-Regression"]},{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 38.5 s, sys: 531 ms, total: 39.1 s\n","Wall time: 19.6 s\n","{'C': 2, 'max_iter': 500}\n","0.9356352036029274\n","0.8042907180385289\n"]}],"source":["from sklearn.linear_model import LogisticRegression\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.3, random_state=42)\n","logreg = LogisticRegression()\n","grid_params = [{'C' : [0.5, 1, 2, 10],\n","                'max_iter': [500, 1000]}]\n","clf = GridSearchCV(estimator = logreg, param_grid = grid_params, cv = 15, verbose = 0)\n","%time clf.fit(X_train, y_train)\n","\n","pred_train = clf.predict(X_train)\n","pred_test = clf.predict(X_test)\n","\n","print(clf.best_params_)\n","print(accuracy_score(y_train, pred_train))\n","print(accuracy_score(y_test, pred_test))\n","\n","df.loc['Log-reg'] = [clf.best_params_, \n","                     100*accuracy_score(y_train, pred_train), confusion_matrix(y_train, pred_train), \n","                     100*accuracy_score(y_test, pred_test), confusion_matrix(y_test, pred_test)]"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3. K-Nearest-Neighbors Classifier"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 2.94 s, sys: 150 ms, total: 3.09 s\n","Wall time: 3 s\n","{'n_neighbors': 100}\n","0.7766935635203603\n","0.7863397548161121\n"]}],"source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","knn = KNeighborsClassifier()\n","grid_params = [{'n_neighbors': [50, 80, 100, 150]}]\n","\n","clf = GridSearchCV(estimator = knn, param_grid = grid_params, cv = 5, verbose = 0)\n","%time clf.fit(X_train, y_train)\n","\n","pred_train = clf.predict(X_train)\n","pred_test = clf.predict(X_test)\n","\n","print(clf.best_params_)\n","print(accuracy_score(y_train, pred_train))\n","print(accuracy_score(y_test, pred_test))\n","\n","df.loc['K-nn'] = [clf.best_params_, \n","                  100*accuracy_score(y_train, pred_train), confusion_matrix(y_train, pred_train), \n","                  100*accuracy_score(y_test, pred_test), confusion_matrix(y_test, pred_test)]"]},{"cell_type":"markdown","metadata":{},"source":["### 3.4. Random Forest Classifier"]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 3min 20s, sys: 495 ms, total: 3min 20s\n","Wall time: 3min 20s\n","{'criterion': 'gini', 'max_depth': 20, 'n_estimators': 300, 'oob_score': True, 'random_state': 88}\n","0.7350347157065116\n","0.717600700525394\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","rfc = RandomForestClassifier()\n","X_train, X_test, y_train, y_test = train_test_split(X_C, y, stratify = y, test_size=0.3, random_state=42)\n","grid_params = [{\n","                'n_estimators': [200, 300], #default=10\n","                'criterion': ['gini'], #default=”gini”\n","                'max_depth': [5, 10, 20], #default=None\n","                'oob_score': [True], #default=False\n","                'random_state': [0, 42, 88]\n","                 }]\n","\n","clf = GridSearchCV(estimator = rfc, param_grid = grid_params, cv = 5, verbose = 0)\n","%time clf.fit(X_train, y_train)\n","\n","pred_train = clf.predict(X_train)\n","pred_test = clf.predict(X_test)\n","\n","print(clf.best_params_)\n","print(accuracy_score(y_train, pred_train))\n","print(accuracy_score(y_test, pred_test))\n","\n","df.loc['rfc'] = [clf.best_params_, \n","                 100*accuracy_score(y_train, pred_train), confusion_matrix(y_train, pred_train), \n","                 100*accuracy_score(y_test, pred_test), confusion_matrix(y_test, pred_test)]"]},{"cell_type":"markdown","metadata":{},"source":["### 3.5. XGB Classifier."]},{"cell_type":"code","execution_count":24,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 16min 39s, sys: 6.9 s, total: 16min 46s\n","Wall time: 4min 18s\n"]},{"data":{"text/plain":["GridSearchCV(cv=5,\n","             estimator=XGBClassifier(base_score=None, booster=None,\n","                                     colsample_bylevel=None,\n","                                     colsample_bynode=None,\n","                                     colsample_bytree=None, gamma=None,\n","                                     gpu_id=None, importance_type='gain',\n","                                     interaction_constraints=None,\n","                                     learning_rate=None, max_delta_step=None,\n","                                     max_depth=None, min_child_weight=None,\n","                                     missing=nan, monotone_constraints=None,\n","                                     n_estimators=100, n_jobs=None,\n","                                     num_parallel_tree=None, random_state=None,\n","                                     reg_alpha=None, reg_lambda=None,\n","                                     scale_pos_weight=None, subsample=None,\n","                                     tree_method=None, validate_parameters=None,\n","                                     verbosity=None),\n","             param_grid=[{'learning_rate': [0.05, 0.1], 'max_depth': [5, 7, 10],\n","                          'n_estimators': [500, 1000, 5000]}])"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["from xgboost import XGBClassifier\n","\n","xgb = XGBClassifier()\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.3, random_state=42)\n","grid_params = [{\n","                'n_estimators': [500, 1000, 5000],\n","                'learning_rate': [.05, 0.1],\n","                'max_depth': [5, 7, 10],\n","            }]\n","clf = GridSearchCV(estimator = xgb, param_grid = grid_params, cv = 5, verbose = 0)\n","%time clf.fit(X_train, y_train,eval_set=[(X_train, y_train), (X_test, y_test)], early_stopping_rounds = 20, verbose = 0)"]},{"cell_type":"code","execution_count":25,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 500}\n","0.7350347157065116\n","0.717600700525394\n"]}],"source":["print(clf.best_params_)\n","print(accuracy_score(y_train, pred_train))\n","print(accuracy_score(y_test, pred_test))\n","\n","df.loc['XGB-simple'] = [clf.best_params_, \n","                         100*accuracy_score(y_train, pred_train), confusion_matrix(y_train, pred_train), \n","                         100*accuracy_score(y_test, pred_test), confusion_matrix(y_test, pred_test)]"]},{"cell_type":"markdown","metadata":{},"source":["#### XGB with complexity-model"]},{"cell_type":"code","execution_count":26,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","\n","class TextSelector(BaseEstimator, TransformerMixin):\n","    def __init__(self, field):\n","        self.field = field\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, X):\n","        return X[self.field]\n","    \n","class NumberSelector(BaseEstimator, TransformerMixin):\n","    def __init__(self, field):\n","        self.field = field\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, X):\n","        return X[[self.field]]"]},{"cell_type":"code","execution_count":27,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[06:59:07] WARNING: /workspace/src/learner.cc:480: \n","Parameters: { base_estimator, cv, eval_set } might not be used.\n","\n","  This may not be accurate due to some parameters are only used in language bindings but\n","  passed down to XGBoost core.  Or some parameters are not used but slip through this\n","  verification. Please open an issue if you find above cases.\n","\n","\n","Fit&trainning time :  58.100852489471436\n","Training_Accuracy: 98.99%\n","Testing_Accuracy: 76.66%\n"]}],"source":["import time\n","from sklearn.svm import SVC\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","svc = SVC()\n","\n","X_t = train[['text', 'Text_length']]\n","X_train, X_test, y_train, y_test = train_test_split(X_t, \n","                                                    y, \n","                                                    test_size = 0.3, \n","                                                    stratify = y, \n","                                                    random_state = 42)\n","clf = Pipeline([\n","    (\n","        'features', FeatureUnion([\n","        ('text', Pipeline([\n","            ('colext', TextSelector('text')),\n","            ('tfidf', TfidfVectorizer(tokenizer = process_text, stop_words = 'english',\n","                     min_df = .0025, max_df = 0.25, ngram_range = (1, 9) ) ),\n","            ('svd', TruncatedSVD(algorithm ='randomized', n_components = 300) ), #for XGB\n","        ])),\n","        ('words', Pipeline([\n","            ('wordext', NumberSelector('Text_length')),\n","            ('wscaler', StandardScaler()),\n","        ])),            \n","    ])\n","    ),\n","    ('clf', XGBClassifier(eval_set=[(X_train, y_train), (X_test, y_test)],\n","                          max_depth = 8, n_estimators = 500, base_estimator = svc, learning_rate = 0.1, cv = 15))\n","    ])\n","\n","## Fit the model\n","start = time.time()\n","clf.fit(X_train, y_train)\n","pred_train = clf.predict(X_train)\n","pred_test = clf.predict(X_test)\n","print ('Fit&trainning time : ', time.time() - start)\n","\n","train_acc_Xgb2 = accuracy_score(y_train, clf.predict(X_train)) * 100.0 \n","test_acc_Xgb2 = accuracy_score(y_test, pred_test) * 100.0\n","\n","print(\"Training_Accuracy: %.2f%%\" % train_acc_Xgb2)\n","print(\"Testing_Accuracy: %.2f%%\" % test_acc_Xgb2)"]},{"cell_type":"code","execution_count":28,"metadata":{"trusted":true},"outputs":[],"source":["df.loc['XGB-complex'] = [\"\", \n","                         100*accuracy_score(y_train, pred_train), confusion_matrix(y_train, pred_train), \n","                         100*accuracy_score(y_test, pred_test), confusion_matrix(y_test, pred_test)]"]},{"cell_type":"markdown","metadata":{},"source":["### 3.6. Adaboost"]},{"cell_type":"code","execution_count":29,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 1min 2s, sys: 48.6 ms, total: 1min 2s\n","Wall time: 1min 1s\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>best_params</th>\n","      <th>train_acc_%</th>\n","      <th>train_conf_matrix</th>\n","      <th>test_acc_%</th>\n","      <th>test_conf_matrix</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>SVM</th>\n","      <td>{'C': 1, 'kernel': 'linear'}</td>\n","      <td>95.027210</td>\n","      <td>[[2994, 45], [220, 2070]]</td>\n","      <td>81.042032</td>\n","      <td>[[1178, 125], [308, 673]]</td>\n","    </tr>\n","    <tr>\n","      <th>MultiNB</th>\n","      <td>{'alpha': 1}</td>\n","      <td>90.392194</td>\n","      <td>[[2917, 122], [390, 1900]]</td>\n","      <td>80.253940</td>\n","      <td>[[1122, 181], [270, 711]]</td>\n","    </tr>\n","    <tr>\n","      <th>Log-reg</th>\n","      <td>{'C': 2, 'max_iter': 500}</td>\n","      <td>93.563520</td>\n","      <td>[[2991, 48], [295, 1995]]</td>\n","      <td>80.429072</td>\n","      <td>[[1180, 123], [324, 657]]</td>\n","    </tr>\n","    <tr>\n","      <th>K-nn</th>\n","      <td>{'n_neighbors': 100}</td>\n","      <td>77.669356</td>\n","      <td>[[2649, 390], [800, 1490]]</td>\n","      <td>78.633975</td>\n","      <td>[[1122, 181], [307, 674]]</td>\n","    </tr>\n","    <tr>\n","      <th>rfc</th>\n","      <td>{'criterion': 'gini', 'max_depth': 20, 'n_esti...</td>\n","      <td>73.503472</td>\n","      <td>[[3039, 0], [1412, 878]]</td>\n","      <td>71.760070</td>\n","      <td>[[1292, 11], [634, 347]]</td>\n","    </tr>\n","    <tr>\n","      <th>XGB-simple</th>\n","      <td>{'learning_rate': 0.1, 'max_depth': 10, 'n_est...</td>\n","      <td>73.503472</td>\n","      <td>[[3039, 0], [1412, 878]]</td>\n","      <td>71.760070</td>\n","      <td>[[1292, 11], [634, 347]]</td>\n","    </tr>\n","    <tr>\n","      <th>XGB-complex</th>\n","      <td></td>\n","      <td>98.986677</td>\n","      <td>[[3028, 11], [43, 2247]]</td>\n","      <td>76.663748</td>\n","      <td>[[1103, 200], [333, 648]]</td>\n","    </tr>\n","    <tr>\n","      <th>Ada_Bst-complex</th>\n","      <td></td>\n","      <td>77.387878</td>\n","      <td>[[2703, 336], [869, 1421]]</td>\n","      <td>74.781086</td>\n","      <td>[[1119, 184], [392, 589]]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                       best_params  \\\n","SVM                                   {'C': 1, 'kernel': 'linear'}   \n","MultiNB                                               {'alpha': 1}   \n","Log-reg                                  {'C': 2, 'max_iter': 500}   \n","K-nn                                          {'n_neighbors': 100}   \n","rfc              {'criterion': 'gini', 'max_depth': 20, 'n_esti...   \n","XGB-simple       {'learning_rate': 0.1, 'max_depth': 10, 'n_est...   \n","XGB-complex                                                          \n","Ada_Bst-complex                                                      \n","\n","                 train_acc_%           train_conf_matrix  test_acc_%  \\\n","SVM                95.027210   [[2994, 45], [220, 2070]]   81.042032   \n","MultiNB            90.392194  [[2917, 122], [390, 1900]]   80.253940   \n","Log-reg            93.563520   [[2991, 48], [295, 1995]]   80.429072   \n","K-nn               77.669356  [[2649, 390], [800, 1490]]   78.633975   \n","rfc                73.503472    [[3039, 0], [1412, 878]]   71.760070   \n","XGB-simple         73.503472    [[3039, 0], [1412, 878]]   71.760070   \n","XGB-complex        98.986677    [[3028, 11], [43, 2247]]   76.663748   \n","Ada_Bst-complex    77.387878  [[2703, 336], [869, 1421]]   74.781086   \n","\n","                          test_conf_matrix  \n","SVM              [[1178, 125], [308, 673]]  \n","MultiNB          [[1122, 181], [270, 711]]  \n","Log-reg          [[1180, 123], [324, 657]]  \n","K-nn             [[1122, 181], [307, 674]]  \n","rfc               [[1292, 11], [634, 347]]  \n","XGB-simple        [[1292, 11], [634, 347]]  \n","XGB-complex      [[1103, 200], [333, 648]]  \n","Ada_Bst-complex  [[1119, 184], [392, 589]]  "]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.ensemble import AdaBoostClassifier\n","\n","X_t = train[['text', 'Numb_words']]\n","X_train, X_test, y_train, y_test = train_test_split(X_t, \n","                                                    y, \n","                                                    test_size = 0.3, \n","                                                    stratify = y, \n","                                                    random_state = 42)\n","\n","clf = Pipeline([\n","    (\n","        'features', FeatureUnion([\n","        ('text', Pipeline([\n","            ('colext', TextSelector('text')),\n","            ('tfidf', TfidfVectorizer(tokenizer = process_text, stop_words = 'english',\n","                     min_df = .0025, max_df = 0.25, ngram_range = (1, 5) ) ),\n","            ('svd', TruncatedSVD(algorithm ='randomized', n_components = 300) ), \n","        ])),\n","        ('words', Pipeline([\n","            ('wordext', NumberSelector('Numb_words')),\n","            ('wscaler', StandardScaler()),\n","        ])),            \n","    ])\n","    ),\n","    ('clf', AdaBoostClassifier(n_estimators = 300, learning_rate = 0.1)),\n","    ])\n","\n","start = time.time()\n","%time clf.fit(X_train, y_train)\n","pred_train = clf.predict(X_train)\n","pred_test = clf.predict(X_test)\n","\n","df.loc['Ada_Bst-complex'] = [\"\", \n","                             100*accuracy_score(y_train, pred_train), confusion_matrix(y_train, pred_train), \n","                             100*accuracy_score(y_test, pred_test), confusion_matrix(y_test, pred_test)]\n","df"]},{"cell_type":"markdown","metadata":{},"source":["### 4. Submission the best_model"]},{"cell_type":"code","execution_count":30,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["array([0, 1, 1, ..., 1, 1, 0])"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["path = r'../input/nlp-getting-started'\n","test_df = pd.read_csv(path + '/test.csv')\n","X = tfidf_vectorizer.transform(test_df['text']) \n","preds = clf_svm.predict(X)\n","preds"]},{"cell_type":"code","execution_count":31,"metadata":{"trusted":true},"outputs":[],"source":["sub_df = pd.DataFrame({'id': test_df['id'], 'target': preds})\n","sub_df.to_csv('submit_ML_model.csv')"]},{"cell_type":"markdown","metadata":{},"source":["### Final results.\n","80.25 % acc"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}
