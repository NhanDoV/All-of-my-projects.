{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import string\nimport pandas as pd\nimport time\n\n## import modules for NLP\nimport nltk\nfrom nltk.corpus import stopwords\n\n## import the modules for train & test data\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import accuracy_score\n\n## word2vec with gensim; press pip install -U gensim before importing\nimport gensim\n\n## Otherwise\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.embeddings import Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom keras.preprocessing.text import one_hot","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading & viewing dataset.\n- References [link_1](https://github.com/Nhan121/Kaggle-6-first-projects/blob/master/NLP_Text_Classification/NLP_Text_classification.ipynb)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = r'../input/nlp-getting-started'\ntrain_df = pd.read_csv(path + '/train.csv')\ntest_df = pd.read_csv(path + '/test.csv')\ntrain_df.head()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 1. Model 1. Only using One-hot Encoding\n### 1.1. One-hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_size = 0.35\ntext = list(train_df['text'])\nvocab_size = 50000\n\nencoded_docs = [one_hot(d, vocab_size) for d in text]\nprint(encoded_docs[:10])","execution_count":3,"outputs":[{"output_type":"stream","text":"[[902, 10396, 48308, 15005, 20564, 5275, 35195, 26550, 15270, 14367, 49640, 8031, 41777], [38776, 4518, 46920, 31425, 43419, 11088, 44938], [41777, 33062, 38696, 13990, 33219, 4316, 47688, 48308, 48056, 467, 22366, 20371, 10097, 3729, 43488, 41712, 21047, 4316, 20566, 44982, 48308, 11870], [42587, 43490, 43837, 36338, 14427, 43488, 44982, 4316, 2529], [27160, 13762, 14138, 35195, 18371, 27738, 22141, 36689, 39232, 47402, 27738, 14427, 46582, 27862, 18452, 16905], [33906, 21225, 2529, 25498, 42633, 40256, 4316, 19610, 17944, 29419, 13990, 33147, 27639, 4518, 25497, 14427], [48722, 34631, 30357, 5867, 40749, 26399, 10343, 5275, 37765, 4316, 493, 39698, 16203, 2986], [37127, 10292, 11207, 5275, 15005, 15983, 13110, 17500, 36123, 34332, 18452, 4518, 4316, 15005, 19627], [24745, 13440, 1561, 43488, 45217, 24046, 4316, 15005, 1041, 16366, 15005, 32817], [37127, 36971, 48439, 15005, 26136, 37598, 4442, 13990, 902, 14539]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Checking the max-length of text"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Numb_words'] = train_df['text'].str.split().map(lambda x: len(x))\ntest_df['Numb_words'] = test_df['text'].str.split().map(lambda x: len(x))\ntrain_df['Numb_words'].max(), test_df['Numb_words'].max()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"(31, 31)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length = 31\nembedding_dim = 32\ntrunc_type='post'\n\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n\nprint(padded_docs)\npadded_docs.shape","execution_count":5,"outputs":[{"output_type":"stream","text":"[[  902 10396 48308 ...     0     0     0]\n [38776  4518 46920 ...     0     0     0]\n [41777 33062 38696 ...     0     0     0]\n ...\n [21256 31929   681 ...     0     0     0]\n [ 8317   216 31009 ...     0     0     0]\n [15005 21245  3473 ...     0     0     0]]\n","name":"stdout"},{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"(7613, 31)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Lauching model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Flatten\nfrom keras.layers import Dense, Dropout, Activation, Conv1D, GlobalMaxPooling1D, MaxPooling1D\n\ny = train_df.target.to_numpy()\nx_train, x_test, y_train, y_test = train_test_split(padded_docs, y, test_size=test_size, \n                                                    stratify = y, random_state = 42)\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 120, input_length=max_length))\nmodel.add(Dropout(0.1))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])\nmodel.summary()","execution_count":6,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 31, 120)           6000000   \n_________________________________________________________________\ndropout (Dropout)            (None, 31, 120)           0         \n_________________________________________________________________\nflatten (Flatten)            (None, 3720)              0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 3721      \n=================================================================\nTotal params: 6,003,721\nTrainable params: 6,003,721\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train, y_train, \n                    epochs = 5, batch_size = 128,\n                    verbose = 2, validation_data = (x_test, y_test))","execution_count":7,"outputs":[{"output_type":"stream","text":"Epoch 1/5\n39/39 - 3s - loss: 0.6638 - acc: 0.5918 - val_loss: 0.6364 - val_acc: 0.6795\nEpoch 2/5\n39/39 - 2s - loss: 0.5571 - acc: 0.7902 - val_loss: 0.5539 - val_acc: 0.7565\nEpoch 3/5\n39/39 - 2s - loss: 0.4027 - acc: 0.8775 - val_loss: 0.4902 - val_acc: 0.7797\nEpoch 4/5\n39/39 - 2s - loss: 0.2646 - acc: 0.9281 - val_loss: 0.4677 - val_acc: 0.7962\nEpoch 5/5\n39/39 - 2s - loss: 0.1704 - acc: 0.9602 - val_loss: 0.4644 - val_acc: 0.7992\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluate accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"score, tr_acc = model.evaluate(x_train, y_train)\nscore, t_acc = model.evaluate(x_test, y_test)\n\nprint('Train accuracy:', tr_acc)\nprint('Test accuracy:', t_acc)","execution_count":8,"outputs":[{"output_type":"stream","text":"155/155 [==============================] - 0s 2ms/step - loss: 0.1242 - acc: 0.9749\n84/84 [==============================] - 0s 2ms/step - loss: 0.4644 - acc: 0.7992\nTrain accuracy: 0.9749393463134766\nTest accuracy: 0.799249529838562\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### 1.2. Submit model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ntext = list(test_df['text'])\nencoded_docs = [one_hot(d, vocab_size) for d in text]\npadded_docs = pad_sequences(encoded_docs, maxlen = max_length, padding='post')\nprint(padded_docs.shape)\npreds = model.predict(padded_docs)\npreds = np.round(preds).ravel()","execution_count":9,"outputs":[{"output_type":"stream","text":"(3263, 31)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.DataFrame({'id': test_df['id'], 'target': preds})\nsub_df.to_csv('submit_DL_model_1.csv')","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final results.\n79.85 % acc"},{"metadata":{},"cell_type":"markdown","source":"## 2. Model 2. LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom tqdm import tqdm\nfrom nltk.tokenize import word_tokenize\n\nstop = set(stopwords.words('english'))\n\n## nltk.download('stopwords') and nltk.download('punkt') before running\n\ndef create_corpus(data):\n    corpus=[]\n    for tweet in tqdm(data['text']):\n        words = [word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\n\nsentences = train_df['text']\ncorpus = create_corpus(train_df)","execution_count":11,"outputs":[{"output_type":"stream","text":"100%|██████████| 7613/7613 [00:03<00:00, 2524.00it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 32\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences = tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad = pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","execution_count":13,"outputs":[{"output_type":"stream","text":"Number of unique words: 15013\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"- Here, we will use `glove.6B.100d.txt` to pre-trainned the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dict={}\nwith open(r\"../input/glove6b100dtxt/glove.6B.100d.txt\", 'r', encoding='utf8', errors='ignore') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_dict[word]=vectors\nf.close()","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = len(word_index)+1\nembedding_matrix = np.zeros((num_words, 100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i] = emb_vec","execution_count":15,"outputs":[{"output_type":"stream","text":"100%|██████████| 15013/15013 [00:00<00:00, 241249.77it/s]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"#### Fitting & Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test,y_train,y_test = train_test_split(tweet_pad, y, test_size = 0.35, random_state = 42)\n\nmodel=Sequential()\n\nembedding=Embedding(num_words, 100, embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.1))\nmodel.add(LSTM(64, dropout=0.1, recurrent_dropout=0.1))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\nmodel.summary()\n\nhistory = model.fit(X_train, y_train, batch_size = 64, epochs = 13, validation_data = (X_test, y_test), verbose=2)\n\nscore, tr_acc = model.evaluate(X_train, y_train)\nscore, t_acc = model.evaluate(X_test, y_test)\n\nprint('Train accuracy:', tr_acc)\nprint('Test accuracy:', t_acc)","execution_count":16,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 32, 100)           1501400   \n_________________________________________________________________\nspatial_dropout1d (SpatialDr (None, 32, 100)           0         \n_________________________________________________________________\nlstm (LSTM)                  (None, 64)                42240     \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 1,543,705\nTrainable params: 42,305\nNon-trainable params: 1,501,400\n_________________________________________________________________\nEpoch 1/13\n78/78 - 5s - loss: 0.5526 - acc: 0.7259 - val_loss: 0.4657 - val_acc: 0.7970\nEpoch 2/13\n78/78 - 5s - loss: 0.4713 - acc: 0.7910 - val_loss: 0.4347 - val_acc: 0.8049\nEpoch 3/13\n78/78 - 5s - loss: 0.4526 - acc: 0.7916 - val_loss: 0.4269 - val_acc: 0.8116\nEpoch 4/13\n78/78 - 5s - loss: 0.4377 - acc: 0.8046 - val_loss: 0.4225 - val_acc: 0.8101\nEpoch 5/13\n78/78 - 5s - loss: 0.4248 - acc: 0.8173 - val_loss: 0.4216 - val_acc: 0.8135\nEpoch 6/13\n78/78 - 5s - loss: 0.4242 - acc: 0.8094 - val_loss: 0.4210 - val_acc: 0.8165\nEpoch 7/13\n78/78 - 5s - loss: 0.4181 - acc: 0.8193 - val_loss: 0.4449 - val_acc: 0.8191\nEpoch 8/13\n78/78 - 5s - loss: 0.4120 - acc: 0.8242 - val_loss: 0.4186 - val_acc: 0.8176\nEpoch 9/13\n78/78 - 5s - loss: 0.4016 - acc: 0.8250 - val_loss: 0.4147 - val_acc: 0.8214\nEpoch 10/13\n78/78 - 5s - loss: 0.3967 - acc: 0.8272 - val_loss: 0.4150 - val_acc: 0.8146\nEpoch 11/13\n78/78 - 5s - loss: 0.3831 - acc: 0.8359 - val_loss: 0.4315 - val_acc: 0.8034\nEpoch 12/13\n78/78 - 5s - loss: 0.3716 - acc: 0.8444 - val_loss: 0.4208 - val_acc: 0.8154\nEpoch 13/13\n78/78 - 5s - loss: 0.3696 - acc: 0.8440 - val_loss: 0.4254 - val_acc: 0.8191\n155/155 [==============================] - 1s 9ms/step - loss: 0.3222 - acc: 0.8692\n84/84 [==============================] - 1s 8ms/step - loss: 0.4254 - acc: 0.8191\nTrain accuracy: 0.8692401051521301\nTest accuracy: 0.8191369771957397\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Submit your result."},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = test_df['text']\ncorpus = create_corpus(test_df)\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences = tokenizer_obj.texts_to_sequences(corpus)\ntweet_pad = pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","execution_count":17,"outputs":[{"output_type":"stream","text":"100%|██████████| 3263/3263 [00:01<00:00, 2484.09it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(tweet_pad)\npreds = np.round(preds).ravel()\nsub_df = pd.DataFrame({'id': test_df['id'], 'target': preds})\nsub_df.to_csv('submit_DL_model_2_LSTM.csv')","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Result\n79.65 % acc"},{"metadata":{},"cell_type":"markdown","source":"#### Comment\n- We need clean text-data carefully before using Deep-Learning model. \n- The original-text contains something that make the Embeddings confusing.\n- When we have pre-trained embeddings, doing standard preprocessing steps might not be a good idea because some of the valuable information can be lost. It is better to get vocabulary as close to embeddings as possible."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}